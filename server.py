from flask import Flask, send_from_directory, jsonify, request
from flask_cors import CORS
import os
from datetime import datetime
import csv
import logging
import re
import random
import requests
import json

import glob
from collections import defaultdict

# === æ•°æ®æ£€ç´¢ç³»ç»Ÿ ===
DATA_DIR = os.path.join(os.path.dirname(__file__), 'data')

# æ•°æ®æ–‡ä»¶ç´¢å¼•ï¼ˆç¼“å­˜ï¼‰
data_index = {}


app = Flask(__name__, static_folder='static')
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# å°è¯•å¯¼å…¥ pandasï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ csv å›é€€å®ç°
try:
    import pandas as pd
except Exception:
    pd = None


def _parse_bysj_to_ym(s):
    """æŠŠ '2023å¹´1æœˆ' æ ·å¼è½¬æ¢ä¸º '2023-01'ï¼Œå¤±è´¥åˆ™è¿”å›åŸå§‹å­—ç¬¦ä¸²ã€‚"""
    if not s:
        return s
    m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ", str(s))
    if m:
        return f"{m.group(1)}-{int(m.group(2)):02d}"
    return str(s)


def load_air_monthly_summary(csv_filename='data/ç©ºæ°”æ±¡æŸ“ç‰©å¹³å‡æµ“åº¦æƒ…å†µè¡¨(0-512).csv'):
    """è¯»å– CSVï¼Œè¿”å›æŒ‰æœˆèšåˆçš„ chart_dataã€overviewã€table_headerã€table_dataã€‚

    å®ç°ç»†èŠ‚ï¼šä¼˜å…ˆä½¿ç”¨ pandas è¯»å–ä¸èšåˆï¼›å¦‚æœæ²¡æœ‰ pandasï¼Œåˆ™ç”¨ csv.DictReader æ‰‹åŠ¨èšåˆã€‚
    è¿”å›çš„ labels ä¸º ['1æœˆ','2æœˆ',...]ï¼ˆæŒ‰æ‰€é€‰æ—¶é—´æ®µé¡ºåºï¼‰ï¼Œdatasets ä¸ä¹‹å‰æ¥å£å…¼å®¹ã€‚
    """
    csv_path = os.path.join(os.path.dirname(__file__), csv_filename)

    rows = []
    if pd is not None:
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except Exception:
            # å…¼å®¹æ²¡æœ‰æŒ‡å®šç¼–ç æˆ–æœ‰ BOM çš„æƒ…å†µ
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
        # æ¸…ç†åˆ—å
        df.columns = [c.strip() for c in df.columns]
        # åªä¿ç•™ç«™ç‚¹è¡Œï¼ˆæ’é™¤å‡å€¼è¡Œï¼‰ï¼Œé¿å…é‡å¤è®¡ç®—
        station_rows = df[df['xsq'].astype(str).str.strip() != 'å‡å€¼'].copy()
        if station_rows.empty:
            station_rows = df.copy()
        # month åˆ—
        station_rows['month'] = station_rows['bysj'].apply(_parse_bysj_to_ym)
        # å¼ºåˆ¶è½¬æ•°å­—
        for col in ['pm25', 'pm10', 'o3']:
            if col in station_rows.columns:
                station_rows[col] = pd.to_numeric(station_rows[col], errors='coerce')
            else:
                station_rows[col] = pd.NA
        # æŒ‰æœˆèšåˆå‡å€¼
        grouped = station_rows.groupby('month', sort=True).agg({
            'pm25': 'mean',
            'pm10': 'mean',
            'o3': 'mean'
        })
        grouped = grouped.sort_index()

        # è®¡ç®—æ¯æœˆæœ€é«˜ PM2.5 çš„ç«™ç‚¹
        top_stations = {}
        for month, g in station_rows.groupby('month'):
            g2 = g.copy()
            g2['pm25'] = pd.to_numeric(g2['pm25'], errors='coerce')
            if not g2['pm25'].dropna().empty:
                idx = g2['pm25'].idxmax()
                top_stations[month] = str(g2.loc[idx, 'xsq'])
            else:
                top_stations[month] = ''

        months = grouped.index.tolist()
        labels = [f"{int(m.split('-')[1])}æœˆ" if isinstance(m, str) and '-' in m else str(m) for m in months]
        pm25 = grouped['pm25'].round().fillna(0).astype(int).tolist()
        o3 = grouped['o3'].round().fillna(0).astype(int).tolist()
        pm10 = grouped['pm10'].round().fillna(0).astype(int).tolist()

    else:
        # fallback: ä½¿ç”¨ csv.DictReader æ‰‹åŠ¨èšåˆ
        if not os.path.exists(csv_path):
            return {}, {}, [], []
        with open(csv_path, newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for r in reader:
                rows.append(r)
        # è¿‡æ»¤æ‰ xsq == 'å‡å€¼'
        rows = [r for r in rows if r.get('xsq', '').strip() != 'å‡å€¼'] or rows
        monthly = {}
        top_stations = {}
        for r in rows:
            month = _parse_bysj_to_ym(r.get('bysj', ''))
            try:
                pm25_v = float(r.get('pm25') or 0)
            except Exception:
                pm25_v = 0
            try:
                pm10_v = float(r.get('pm10') or 0)
            except Exception:
                pm10_v = 0
            try:
                o3_v = float(r.get('o3') or 0)
            except Exception:
                o3_v = 0
            if month not in monthly:
                monthly[month] = {'pm25_sum': 0.0, 'pm10_sum': 0.0, 'o3_sum': 0.0, 'count': 0}
            monthly[month]['pm25_sum'] += pm25_v
            monthly[month]['pm10_sum'] += pm10_v
            monthly[month]['o3_sum'] += o3_v
            monthly[month]['count'] += 1
            # top station
            cur_top = top_stations.get(month)
            if cur_top is None:
                top_stations[month] = (r.get('xsq', ''), pm25_v)
            else:
                if pm25_v > cur_top[1]:
                    top_stations[month] = (r.get('xsq', ''), pm25_v)
        months = sorted(monthly.keys())
        labels = [f"{int(m.split('-')[1])}æœˆ" if isinstance(m, str) and '-' in m else str(m) for m in months]
        pm25 = [int(round(monthly[m]['pm25_sum'] / monthly[m]['count'])) if monthly[m]['count'] else 0 for m in months]
        pm10 = [int(round(monthly[m]['pm10_sum'] / monthly[m]['count'])) if monthly[m]['count'] else 0 for m in months]
        o3 = [int(round(monthly[m]['o3_sum'] / monthly[m]['count'])) if monthly[m]['count'] else 0 for m in months]
        # convert top_stations values to names
        top_stations = {m: top_stations[m][0] if isinstance(top_stations[m], tuple) else '' for m in months}

    # è®¡ç®—ç´¯è®¡å€¼ï¼ˆæŒ‰æœˆä»½é¡ºåºï¼‰
    cum_pm25 = []
    cum_o3 = []
    cum_pm10 = []
    s_pm25 = s_o3 = s_pm10 = 0
    for a, b, c in zip(pm25, o3, pm10):
        s_pm25 += a
        s_o3 += b
        s_pm10 += c
        cum_pm25.append(s_pm25)
        cum_o3.append(s_o3)
        cum_pm10.append(s_pm10)

    # ç®€å•çš„ç©ºæ°”è´¨é‡æè¿°ï¼šæ ¹æ® PM2.5 å¹´å¹³å‡
    overall_pm25_avg = int(round(sum(pm25) / len(pm25))) if pm25 else 0
    if overall_pm25_avg <= 35:
        quality_label = 'è‰¯å¥½'
    elif overall_pm25_avg <= 75:
        quality_label = 'è½»åº¦æ±¡æŸ“'
    else:
        quality_label = 'æ±¡æŸ“'

    overview = {
        'recordCount': None,  # è®°å½•æ•°è§†æ–‡ä»¶è€Œå®šï¼›åœ¨ pandas åˆ†æ”¯æˆ‘ä»¬å¯ä»¥æ›´ç²¾ç¡®
        'stationCount': None,
        'timeSpan': '',
        'avgQuality': quality_label,
        'pm25Avg': f"{overall_pm25_avg} Î¼g/mÂ³",
        'o3Avg': f"{int(round(sum(o3) / len(o3))) if o3 else 0} Î¼g/mÂ³",
        'pm10Avg': f"{int(round(sum(pm10) / len(pm10))) if pm10 else 0} Î¼g/mÂ³"
    }

    # å°è¯•ç”¨ pandas æ—¶å¡«å……æ›´ç²¾ç¡®çš„ recordCount ä¸ stationCount ä¸ timeSpan
    if pd is not None:
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except Exception:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
        df.columns = [c.strip() for c in df.columns]
        station_rows = df[df['xsq'].astype(str).str.strip() != 'å‡å€¼']
        overview['recordCount'] = int(len(station_rows))
        overview['stationCount'] = int(station_rows['xsq'].nunique())
        months_full = sorted(list({_parse_bysj_to_ym(x) for x in station_rows['bysj'].tolist()}))
        overview['timeSpan'] = f"{months_full[0]} è‡³ {months_full[-1]}" if months_full else ''
    else:
        # åœ¨ csv å›é€€åˆ†æ”¯ä¸­ç”¨ rows å¡«å……
        if rows:
            overview['recordCount'] = len(rows)
            overview['stationCount'] = len(set(r.get('xsq','') for r in rows))
            months_full = sorted(list({_parse_bysj_to_ym(r.get('bysj','')) for r in rows}))
            overview['timeSpan'] = f"{months_full[0]} è‡³ {months_full[-1]}" if months_full else ''

    # æ„é€  chart_data ä¸ table
    chart_data = {
        'labels': labels,
        'datasets': [
            {
                'label': 'ç´¯è®¡ç»†é¢—ç²’ç‰©(PM2.5) Î¼g/mÂ³',
                'data': pm25,
                'borderColor': '#00F0FF',
                'backgroundColor': 'rgba(0, 240, 255, 0.1)',
                'borderWidth': 2,
                'tension': 0.4,
                'fill': True
            },
            {
                'label': 'ç´¯è®¡è‡­æ°§(Oâ‚ƒ) Î¼g/mÂ³',
                'data': o3,
                'borderColor': '#FF0080',
                'backgroundColor': 'rgba(255, 0, 128, 0.1)',
                'borderWidth': 2,
                'tension': 0.4,
                'fill': True
            },
            {
                'label': 'ç´¯è®¡å¯å¸å…¥ç‰©(PM10) Î¼g/mÂ³',
                'data': pm10,
                'borderColor': '#39FF14',
                'backgroundColor': 'rgba(57, 255, 20, 0.1)',
                'borderWidth': 2,
                'tension': 0.4,
                'fill': True
            }
        ]
    }

    table_header = ["æ—¶é—´", "æ¯æœˆPM2.5å¹³å‡(Î¼g/mÂ³)", "æ¯æœˆè‡­æ°§å¹³å‡(Î¼g/mÂ³)", "æ¯æœˆå¯å¸å…¥ç‰©å¹³å‡(Î¼g/mÂ³)",
                    "ç´¯è®¡PM2.5", "ç´¯è®¡è‡­æ°§", "ç´¯è®¡å¯å¸å…¥ç‰©", "ç›‘æµ‹ç«™ç‚¹"]

    table_data = []
    for m, lab, a, b, c, cp, co, ck in zip(months, labels, pm25, o3, pm10, cum_pm25, cum_o3, cum_pm10):
        table_data.append([m, str(a), str(b), str(c), str(cp), str(co), str(ck), top_stations.get(m, '')])

    return chart_data, overview, table_header, table_data


def _parse_numeric_from_str(s):
    """ä»å­—ç¬¦ä¸²ä¸­æå–æ•°å€¼æˆ–åŒºé—´å¹¶è¿”å›å¹³å‡å€¼ï¼ˆfloatï¼‰ã€‚"""
    if s is None:
        return None
    s = str(s)
    # å»æ‰ç™¾åˆ†å·ç­‰éæ•°å­—ç¬¦å·ï¼ˆä¿ç•™ . å’Œ -ï¼‰
    # æå–æ‰€æœ‰æµ®ç‚¹æ•°
    nums = re.findall(r"[-+]?\d*\.?\d+", s)
    nums = [float(n) for n in nums] if nums else []
    if not nums:
        return None
    if len(nums) == 1:
        return nums[0]
    # è‹¥æ˜¯åŒºé—´å–å¹³å‡
    return sum(nums) / len(nums)


def load_water_monthly_summary(csv_filename='data/å®œæ˜Œå¸‚æ°´è´¨è‡ªåŠ¨ç«™ç›‘æµ‹æƒ…å†µ(0-421).csv'):
    """è¯»å–æ°´è´¨ç›‘æµ‹ CSV å¹¶æŒ‰æœˆèšåˆ pHã€æº¶è§£æ°§ã€æ°¨æ°® ç­‰æŒ‡æ ‡ã€‚

    è¿”å› (chart_data, overview, table_header, table_data)
    chart_data.datasets ä½¿ç”¨æµ®ç‚¹æ•°ï¼ˆä¿ç•™ä¸€ä½å°æ•°ï¼‰ï¼Œlabels ä¸º ['1æœˆ', ...]
    """
    csv_path = os.path.join(os.path.dirname(__file__), csv_filename)
    rows = []
    monthly = {}

    if pd is not None:
        try:
            df = pd.read_csv(csv_path, encoding='utf-8')
        except Exception:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
        df.columns = [c.strip() for c in df.columns]
        # éœ€è¦çš„åˆ—ï¼š bysj (æœˆä»½), bycbxm(è¢«æµ‹å‚æ•°å), cbznd(å€¼æˆ–èŒƒå›´), zdzmc(ç«™ç‚¹), szxz(æ°´è´¨ç±»åˆ«)
        for _, r in df.iterrows():
            month = _parse_bysj_to_ym(r.get('bysj', ''))
            item = str(r.get('bycbxm', '')).strip()
            value = r.get('cbznd', '')
            station = str(r.get('zdzmc', '')).strip() if 'zdzmc' in r.index else str(r.get('stmc', '')).strip()
            sz = str(r.get('szxz', '')).strip() if 'szxz' in r.index else ''
            if not month:
                continue
            if month not in monthly:
                monthly[month] = {'ph': [], 'do': [], 'ammonia': [], 'stations': set(), 'sz_list': []}
            monthly[month]['stations'].add(station)
            if sz:
                monthly[month]['sz_list'].append(sz)
            # ä¼˜å…ˆç”¨ cbznd åˆ—è§£æï¼Œå¦‚æœä¸ºç©ºåˆ™å°è¯•ä» bycbxm ä¸­è§£ææ‹¬å·é‡Œçš„æ•°å€¼
            val = None
            if value and str(value).strip() and str(value).strip() != '--':
                val = _parse_numeric_from_str(value)
            else:
                # å°è¯•ä» bycbxm ä¸­å¯»æ‰¾æ•°å­—
                val = _parse_numeric_from_str(item)
            # æ ¹æ®å‚æ•°ååˆ†é…
            low_item = item.lower()
            if 'ph' in low_item or 'pH' in item or 'é…¸ç¢±' in low_item:
                if val is not None:
                    monthly[month]['ph'].append(val)
            elif 'æº¶è§£æ°§' in item or 'æº¶è§£æ°§' in low_item or 'do' in low_item:
                if val is not None:
                    monthly[month]['do'].append(val)
            elif 'æ°¨æ°®' in item or 'ammonia' in low_item:
                if val is not None:
                    monthly[month]['ammonia'].append(val)
            else:
                # å…¶ä»–å‚æ•°å¿½ç•¥
                pass

    else:
        # fallback csv
        if not os.path.exists(csv_path):
            return {}, {}, [], []
        with open(csv_path, newline='', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for r in reader:
                month = _parse_bysj_to_ym(r.get('bysj', ''))
                item = str(r.get('bycbxm', '')).strip()
                value = r.get('cbznd', '')
                station = str(r.get('zdzmc', '')).strip() or str(r.get('stmc', '')).strip()
                sz = str(r.get('szxz', '')).strip()
                if not month:
                    continue
                if month not in monthly:
                    monthly[month] = {'ph': [], 'do': [], 'ammonia': [], 'stations': set(), 'sz_list': []}
                monthly[month]['stations'].add(station)
                if sz:
                    monthly[month]['sz_list'].append(sz)
                val = None
                if value and str(value).strip() and str(value).strip() != '--':
                    val = _parse_numeric_from_str(value)
                else:
                    val = _parse_numeric_from_str(item)
                low_item = item.lower()
                if 'ph' in low_item or 'pH' in item or 'é…¸ç¢±' in low_item:
                    if val is not None:
                        monthly[month]['ph'].append(val)
                elif 'æº¶è§£æ°§' in item or 'æº¶è§£æ°§' in low_item or 'do' in low_item:
                    if val is not None:
                        monthly[month]['do'].append(val)
                elif 'æ°¨æ°®' in item or 'ammonia' in low_item:
                    if val is not None:
                        monthly[month]['ammonia'].append(val)

    # ç»„ç»‡æŒ‰æ—¶é—´æ’åºçš„ç»“æœ
    months = sorted(monthly.keys())
    labels = [f"{int(m.split('-')[1])}æœˆ" if '-' in m else m for m in months]
    ph_list = []
    do_list = []
    ammonia_list = []
    stations_list = []
    sz_overview = []
    for m in months:
        rec = monthly[m]
        ph_avg = round(sum(rec['ph']) / len(rec['ph']), 2) if rec['ph'] else None
        do_avg = round(sum(rec['do']) / len(rec['do']), 2) if rec['do'] else None
        am_avg = round(sum(rec['ammonia']) / len(rec['ammonia']), 3) if rec['ammonia'] else None
        ph_list.append(ph_avg if ph_avg is not None else 0)
        do_list.append(do_avg if do_avg is not None else 0)
        ammonia_list.append(am_avg if am_avg is not None else 0)
        stations_list.append(','.join(list(rec['stations'])[:1]))
        # ä½¿ç”¨æœ€å¸¸è§çš„æ°´è´¨ç±»åˆ«
        if rec['sz_list']:
            from collections import Counter
            sz_overview.append(Counter(rec['sz_list']).most_common(1)[0][0])
        else:
            sz_overview.append('')

    # ç®€å•æ¦‚è§ˆ
    total_records = sum(len(monthly[m]['stations']) for m in months) if months else 0
    def _avg_nonzero(lst):
        vals = [x for x in lst if x is not None and x != 0]
        return round(sum(vals) / len(vals), 3) if vals else ''

    overview = {
        'recordCount': total_records,
        'monitorPoint': len({s for m in months for s in monthly[m]['stations']}) if months else 0,
        'timeSpan': f"{months[0]} è‡³ {months[-1]}" if months else '',
        'qualifiedRate': '',
        'avgPh': _avg_nonzero(ph_list),
        'avgDo': _avg_nonzero(do_list),
        'avgAmmonia': _avg_nonzero(ammonia_list)
    }

    chart_data = {
        'labels': labels,
        'datasets': [
            {'label': 'pHå€¼', 'data': ph_list, 'borderColor': '#00F0FF', 'fill': True},
            {'label': 'æº¶è§£æ°§(mg/L)', 'data': do_list, 'borderColor': '#39FF14', 'fill': True},
            {'label': 'æ°¨æ°®(mg/L)', 'data': ammonia_list, 'borderColor': '#FF0080', 'fill': True}
        ]
    }

    table_header = ["æ—¶é—´", "pHå€¼", "æº¶è§£æ°§(mg/L)", "æ°¨æ°®(mg/L)", "æ°´è´¨ç±»åˆ«", "ç›‘æµ‹ç‚¹"]
    table_data = []
    for m, lab, p, d, a, sz, st in zip(months, labels, ph_list, do_list, ammonia_list, sz_overview, stations_list):
        table_data.append([m, str(p), str(d), str(a), sz or 'â€”', st or 'â€”'])

    return chart_data, overview, table_header, table_data


# === 1ï¸âƒ£ é™æ€æ–‡ä»¶ï¼šè¿”å›å‰ç«¯é¡µé¢ ===
@app.route('/home')
def index():
    return send_from_directory(app.static_folder, 'index.html')


@app.route('/data_center')
def data_center():
    return send_from_directory(app.static_folder, 'data_center.html')

@app.route('/report')
def report():
    return send_from_directory(app.static_folder, 'report.html')

@app.route('/about')
def about():
    return send_from_directory(app.static_folder, 'about.html')

@app.route('/gpt')
def gpt():
    return send_from_directory(app.static_folder, 'gpt.html')

# === DeepSeek API é…ç½® ===
DEEPSEEK_API_KEY = 'sk-a89f48e8ce9946198f91abceee3f756a'  # ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæˆ–ç›´æ¥å¡«å†™
DEEPSEEK_API_URL = 'https://api.deepseek.com/v1/chat/completions'

# å­˜å‚¨å¯¹è¯å†å²ï¼ˆå®é™…é¡¹ç›®ä¸­å»ºè®®ä½¿ç”¨æ•°æ®åº“æˆ– Redisï¼‰
chat_history = {}

def build_data_index():
    """æ„å»ºæ•°æ®æ–‡ä»¶ç´¢å¼•"""
    global data_index
    if data_index:
        return data_index
    
    data_index = {}
    csv_files = glob.glob(os.path.join(DATA_DIR, '*.csv'))
    
    for csv_file in csv_files:
        filename = os.path.basename(csv_file)
        try:
            if pd is not None:
                df = pd.read_csv(csv_file, encoding='utf-8-sig', nrows=100)  # åªè¯»å‰100è¡Œç”¨äºç´¢å¼•
                df.columns = [c.strip() for c in df.columns]
                
                # æå–å…³é”®ä¿¡æ¯
                columns = list(df.columns)
                sample_data = df.head(5).to_dict('records') if len(df) > 0 else []
                
                data_index[filename] = {
                    'columns': columns,
                    'sample_data': sample_data,
                    'row_count': len(df),
                    'keywords': extract_keywords(filename, columns, sample_data)
                }
            else:
                # æ— pandasæ—¶çš„ç®€å•å¤„ç†
                with open(csv_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    rows = list(reader)[:100]
                    if rows:
                        columns = list(rows[0].keys())
                        data_index[filename] = {
                            'columns': columns,
                            'sample_data': rows[:5],
                            'row_count': len(rows),
                            'keywords': extract_keywords(filename, columns, rows[:5])
                        }
        except Exception as e:
            logging.warning(f"ç´¢å¼•æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
            continue
    
    return data_index

def extract_keywords(filename, columns, sample_data):
    """ä»æ–‡ä»¶åã€åˆ—åå’Œæ•°æ®ä¸­æå–å…³é”®è¯"""
    keywords = set()
    
    # ä»æ–‡ä»¶åæå–
    filename_lower = filename.lower()
    keywords.add(filename_lower.replace('.csv', ''))
    
    # ä»åˆ—åæå–
    for col in columns:
        col_lower = str(col).lower()
        keywords.add(col_lower)
        # æå–ä¸­æ–‡å…³é”®è¯
        if 'äººå£' in col or 'åŸé•‡åŒ–' in col:
            keywords.update(['äººå£', 'åŸé•‡åŒ–', 'äººå£ç»Ÿè®¡'])
        if 'ç©ºæ°”' in col or 'pm' in col_lower or 'pm25' in col_lower or 'pm10' in col_lower:
            keywords.update(['ç©ºæ°”è´¨é‡', 'pm2.5', 'pm10', 'ç©ºæ°”æ±¡æŸ“'])
        if 'æ°´è´¨' in col or 'æ°´' in col:
            keywords.update(['æ°´è´¨', 'æ°´èµ„æº', 'æ°´ç›‘æµ‹'])
        if 'æ°”æ¸©' in col or 'æ¸©åº¦' in col:
            keywords.update(['æ°”æ¸©', 'æ¸©åº¦', 'æ°”è±¡'])
        if 'å­¦ç”Ÿ' in col or 'æ•™è‚²' in col:
            keywords.update(['å­¦ç”Ÿ', 'æ•™è‚²', 'å­¦æ ¡'])
        if 'åŒ»é™¢' in col or 'åŒ»ç–—' in col:
            keywords.update(['åŒ»é™¢', 'åŒ»ç–—', 'å¥åº·'])
        if 'æ—…æ¸¸' in col or 'æ—…è¡Œç¤¾' in col:
            keywords.update(['æ—…æ¸¸', 'æ—…è¡Œç¤¾'])
        if 'æ¶ˆè´¹' in col or 'é›¶å”®' in col:
            keywords.update(['æ¶ˆè´¹', 'é›¶å”®', 'ç»æµ'])
        if 'ä¼ä¸š' in col or 'å·¥ä¸š' in col:
            keywords.update(['ä¼ä¸š', 'å·¥ä¸š', 'ç»æµ'])
    
    return list(keywords)

def search_relevant_data(user_query):
    """æ ¹æ®ç”¨æˆ·é—®é¢˜æœç´¢ç›¸å…³æ•°æ®"""
    query_lower = user_query.lower()
    relevant_files = []
    
    # æ„å»ºç´¢å¼•
    index = build_data_index()
    
    # åŒ¹é…ç›¸å…³æ–‡ä»¶
    for filename, info in index.items():
        score = 0
        keywords = info.get('keywords', [])
        
        # æ£€æŸ¥å…³é”®è¯åŒ¹é…
        for keyword in keywords:
            if keyword in query_lower:
                score += 1
        
        # æ£€æŸ¥åˆ—ååŒ¹é…
        for col in info.get('columns', []):
            col_lower = str(col).lower()
            if any(word in col_lower for word in query_lower.split() if len(word) > 2):
                score += 0.5
        
        if score > 0:
            relevant_files.append((filename, score, info))
    
    # æŒ‰åˆ†æ•°æ’åºï¼Œè¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡ä»¶
    relevant_files.sort(key=lambda x: x[1], reverse=True)
    return relevant_files[:3]

def load_data_context(file_info_list):
    """åŠ è½½ç›¸å…³æ•°æ®æ–‡ä»¶çš„ä¸Šä¸‹æ–‡"""
    context_parts = []
    
    for filename, score, info in file_info_list:
        csv_path = os.path.join(DATA_DIR, filename)
        try:
            if pd is not None:
                df = pd.read_csv(csv_path, encoding='utf-8-sig')
                df.columns = [c.strip() for c in df.columns]
                
                # é™åˆ¶æ•°æ®é‡ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                if len(df) > 50:
                    df_sample = df.head(50)  # åªå–å‰50è¡Œ
                else:
                    df_sample = df
                
                # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                data_text = f"\næ•°æ®æ–‡ä»¶ï¼š{filename}\n"
                data_text += f"åˆ—åï¼š{', '.join(df.columns.tolist())}\n"
                data_text += "æ•°æ®ç¤ºä¾‹ï¼ˆå‰50è¡Œï¼‰ï¼š\n"
                data_text += df_sample.to_string(index=False)
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                numeric_cols = df.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    data_text += f"\n\nç»Ÿè®¡æ‘˜è¦ï¼š\n"
                    for col in numeric_cols[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°å€¼åˆ—
                        data_text += f"{col}: å¹³å‡å€¼={df[col].mean():.2f}, æœ€å¤§å€¼={df[col].max():.2f}, æœ€å°å€¼={df[col].min():.2f}\n"
                
                context_parts.append(data_text)
            else:
                # æ— pandasæ—¶çš„ç®€å•å¤„ç†
                with open(csv_path, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    rows = list(reader)[:50]  # é™åˆ¶50è¡Œ
                    
                    data_text = f"\næ•°æ®æ–‡ä»¶ï¼š{filename}\n"
                    if rows:
                        data_text += f"åˆ—åï¼š{', '.join(rows[0].keys())}\n"
                        data_text += "æ•°æ®ç¤ºä¾‹ï¼ˆå‰50è¡Œï¼‰ï¼š\n"
                        for i, row in enumerate(rows[:10], 1):  # åªæ˜¾ç¤ºå‰10è¡Œ
                            data_text += f"{i}. {dict(row)}\n"
                    
                    context_parts.append(data_text)
        except Exception as e:
            logging.warning(f"åŠ è½½æ•°æ®æ–‡ä»¶ {filename} å¤±è´¥: {str(e)}")
            continue
    
    return "\n".join(context_parts)

# ä¿®æ”¹ chat() å‡½æ•°ï¼Œåœ¨è°ƒç”¨ DeepSeek API ä¹‹å‰æ·»åŠ æ•°æ®æ£€ç´¢
@app.route('/api/chat', methods=['POST'])
def chat():
    """å¤„ç†èŠå¤©è¯·æ±‚ï¼Œè°ƒç”¨ DeepSeek API"""
    try:
        data = request.get_json()
        user_message = data.get('message', '').strip()
        session_id = data.get('session_id', 'default')
        
        if not user_message:
            return jsonify({'error': 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º'}), 400
        
        # è·å–æˆ–åˆå§‹åŒ–å¯¹è¯å†å²
        if session_id not in chat_history:
            chat_history[session_id] = []
        
        # ğŸ” æ£€ç´¢ç›¸å…³æ•°æ®
        relevant_files = search_relevant_data(user_message)
        data_context = ""
        if relevant_files:
            data_context = load_data_context(relevant_files)
            logging.info(f"æ£€ç´¢åˆ° {len(relevant_files)} ä¸ªç›¸å…³æ•°æ®æ–‡ä»¶ï¼š{relevant_files}")
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆåŒ…å«å†å²å¯¹è¯ï¼‰
        messages = chat_history[session_id].copy()
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«æ•°æ®ä¸Šä¸‹æ–‡ï¼‰
        system_content = """ä½ æ˜¯æ•°æ™ºæ¹–åŒ—AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·åˆ†ææ•°æ®ã€è§£ç­”é—®é¢˜ã€‚è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›ç­”ã€‚

é‡è¦æç¤ºï¼š
1. å¦‚æœç”¨æˆ·è¯¢é—®å…³äºæ•°æ®çš„é—®é¢˜ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨æä¾›çš„æœ¬åœ°æ•°æ®æ¥å›ç­”
2. å›ç­”æ—¶è¦å¼•ç”¨å…·ä½“çš„æ•°æ®å€¼å’Œæ•°æ®æ¥æº
3. å¦‚æœæ•°æ®ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
4. å¯ä»¥åŸºäºæ•°æ®è¿›è¡Œç®€å•çš„åˆ†æå’Œè¶‹åŠ¿åˆ¤æ–­"""
        
        # å¦‚æœæœ‰ç›¸å…³æ•°æ®ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæç¤ºè¯ä¸­
        if data_context:
            system_content += f"\n\nä»¥ä¸‹æ˜¯ç›¸å…³çš„æœ¬åœ°æ•°æ®ï¼Œè¯·åŸºäºè¿™äº›æ•°æ®å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n{data_context}"
        
        system_message = {
            "role": "system",
            "content": system_content
        }
        
        # å¦‚æœå†å²è®°å½•ä¸­æ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œåˆ™æ·»åŠ ï¼ˆæ¯æ¬¡æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯ä»¥åŒ…å«æœ€æ–°æ•°æ®ï¼‰
        # ç§»é™¤æ—§çš„ç³»ç»Ÿæ¶ˆæ¯
        messages = [msg for msg in messages if msg.get('role') != 'system']
        messages.insert(0, system_message)
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append({
            "role": "user",
            "content": user_message
        })
        
        # è°ƒç”¨ DeepSeek API
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {DEEPSEEK_API_KEY}'
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 2000,
            "stream": False
        }
        
        response = requests.post(
            DEEPSEEK_API_URL,
            headers=headers,
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            
            # æ›´æ–°å¯¹è¯å†å²ï¼ˆä¸ä¿å­˜ç³»ç»Ÿæ¶ˆæ¯ï¼Œåªä¿å­˜ç”¨æˆ·å’ŒAIçš„å¯¹è¯ï¼‰
            chat_history[session_id].append({
                "role": "user",
                "content": user_message
            })
            chat_history[session_id].append({
                "role": "assistant",
                "content": ai_response
            })
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦
            if len(chat_history[session_id]) > 40:
                chat_history[session_id] = chat_history[session_id][-40:]
            
            return jsonify({
                'response': ai_response,
                'session_id': session_id,
                'data_sources': [f[0] for f in relevant_files] if relevant_files else []  # è¿”å›ä½¿ç”¨çš„æ•°æ®æº
            })
        else:
            error_msg = f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"
            try:
                error_detail = response.json()
                error_msg = error_detail.get('error', {}).get('message', error_msg)
            except:
                pass
            return jsonify({'error': error_msg}), response.status_code
            
    except requests.exceptions.Timeout:
        return jsonify({'error': 'è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•'}), 504
    except requests.exceptions.RequestException as e:
        return jsonify({'error': f'ç½‘ç»œé”™è¯¯: {str(e)}'}), 500
    except Exception as e:
        logging.error(f"Chat error: {str(e)}")
        return jsonify({'error': f'æœåŠ¡å™¨é”™è¯¯: {str(e)}'}), 500

@app.route('/api/chat/clear', methods=['POST'])
def clear_chat_history():
    """æ¸…ç©ºæŒ‡å®šä¼šè¯çš„å¯¹è¯å†å²"""
    try:
        data = request.get_json()
        session_id = data.get('session_id', 'default')
        
        if session_id in chat_history:
            chat_history[session_id] = []
            return jsonify({'success': True, 'message': 'å¯¹è¯å†å²å·²æ¸…ç©º'})
        else:
            return jsonify({'success': True, 'message': 'æ²¡æœ‰å¯¹è¯å†å²éœ€è¦æ¸…ç©º'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# === 2ï¸âƒ£ æ ¸å¿ƒé¢„æµ‹æŒ‡æ ‡æ¥å£ï¼ˆGDPã€å¤±ä¸šç‡ã€æ–°èƒ½æºæ±½è½¦ã€PM2.5ï¼‰===
@app.route('/api/core-indicators', methods=['GET'])
def get_core_indicators():
    """è¿”å›æ ¸å¿ƒé¢„æµ‹æŒ‡æ ‡æ•°æ®ï¼ˆå«è¶‹åŠ¿å›¾æ•°æ®ï¼‰"""
    # å®é™…é¡¹ç›®ä¸­å¯æ›¿æ¢ä¸ºæ•°æ®åº“æŸ¥è¯¢/æ¨¡å‹è®¡ç®—é€»è¾‘
    return jsonify({
        "gdp": {
            "value": 5.8,
            "change": "+0.3%",
            "desc": "é«˜äºå…¨å›½å¹³å‡æ°´å¹³",
            "labels": ['2020', '2021', '2022', '2023', '2024'],
            "data": [3.8, 4.5, 5.2, 5.5, 5.8]
        },
        "unemployment": {
            "value": 4.2,
            "change": "+0.1%",
            "desc": "è¾ƒä¸Šå­£åº¦ç•¥æœ‰ä¸Šå‡",
            "labels": ['2020', '2021', '2022', '2023', '2024'],
            "data": [5.1, 4.8, 4.5, 4.1, 4.2]
        },
        "ev": {
            "value": 12.0,
            "change": "+2.3%",
            "desc": "å—æ–°äº§èƒ½é‡Šæ”¾æ¨åŠ¨",
            "labels": ['2020', '2021', '2022', '2023', '2024'],
            "data": [5.2, 7.8, 8.5, 9.7, 12.0]
        },
        "pm25": {
            "value": 45,
            "change": "-8%",
            "desc": "ç©ºæ°”è´¨é‡æŒç»­æ”¹å–„",
            "labels": ['2020', '2021', '2022', '2023', '2024'],
            "data": [68, 62, 55, 49, 45]
        },
        "update_time": datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    })


# === 3ï¸âƒ£ å¸‚å·GDPé¢„æµ‹æ¥å£ ===
@app.route('/api/city-gdp', methods=['GET'])
def get_city_gdp():
    """è¿”å›å„å¸‚å·GDPå¢é€Ÿé¢„æµ‹æ•°æ®"""
    return jsonify({
        "cities": ['æ­¦æ±‰', 'è¥„é˜³', 'å®œæ˜Œ', 'è†å·', 'é»„å†ˆ', 'å­æ„Ÿ', 'è†é—¨'],
        "actual2023": [6.1, 5.8, 5.5, 5.2, 4.9, 5.0, 5.3],
        "pred2024": [6.5, 6.2, 5.9, 5.6, 5.3, 5.4, 5.7]
    })


# === 4ï¸âƒ£ é‡ç‚¹äº§ä¸šäº§å€¼é¢„æµ‹æ¥å£ ===
@app.route('/api/industries', methods=['GET'])
def get_industries():
    """è¿”å›é‡ç‚¹äº§ä¸šäº§å€¼é¢„æµ‹æ•°æ®"""
    return jsonify([
        {"name": "æ±½è½¦äº§ä¸š", "growth": "+7.2%", "color": "primary", "percent": 72},
        {"name": "å…‰ç”µå­äº§ä¸š", "growth": "+15.8%", "color": "secondary", "percent": 85},
        {"name": "ç”Ÿç‰©åŒ»è¯äº§ä¸š", "growth": "+9.5%", "color": "accent", "percent": 68},
        {"name": "è£…å¤‡åˆ¶é€ äº§ä¸š", "growth": "+6.3%", "color": "success", "percent": 63}
    ])


# === 5ï¸âƒ£ æ°‘ç”Ÿå•†å“ä»·æ ¼é¢„æµ‹æ¥å£ ===
@app.route('/api/commodity-prices', methods=['GET'])
def get_commodity_prices():
    """è¿”å›ä¸»è¦æ°‘ç”Ÿå•†å“ä»·æ ¼æ³¢åŠ¨é¢„æµ‹æ•°æ®"""
    months = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ', '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ']
    return jsonify({
        "months": months,
        "pork": [28, 29, 27, 26, 25, 24, 23, 24, 25, 26, 28, 30],
        "rice": [4.2, 4.2, 4.3, 4.3, 4.4, 4.4, 4.5, 4.5, 4.4, 4.4, 4.3, 4.3],
        "oil": [15.8, 15.9, 16.0, 16.2, 16.3, 16.2, 16.1, 16.0, 15.9, 15.8, 15.8, 15.9]
    })


# === 6ï¸âƒ£ æ•™è‚²åŒ»ç–—èµ„æºé¢„æµ‹æ¥å£ ===
@app.route('/api/edu-med-resources', methods=['GET'])
def get_edu_med_resources():
    """è¿”å›æ•™è‚²åŒ»ç–—èµ„æºä¾›éœ€é¢„æµ‹æ•°æ®"""
    return jsonify({
        "edu_gap": [
            {"city": "æ­¦æ±‰å¸‚", "gap": "+12,500", "color": "danger", "percent": 75},
            {"city": "è¥„é˜³å¸‚", "gap": "+3,200", "color": "warning", "percent": 40},
            {"city": "å®œæ˜Œå¸‚", "gap": "-1,800", "color": "success", "percent": 20}
        ],
        "medical_peak": [
            {"name": "å†¬å­£å‘¼å¸é“ç–¾ç—…", "period": "12æœˆ-1æœˆ", "color": "primary", "percent": 85},
            {"name": "å„¿ç«¥ç–«è‹—æ¥ç§", "period": "9æœˆ-10æœˆ", "color": "secondary", "percent": 65},
            {"name": "ä½“æ£€é«˜å³°æœŸ", "period": "3æœˆ-4æœˆ", "color": "accent", "percent": 50}
        ]
    })




# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
# ä¿ç•™åŸæœ‰æ‰€æœ‰å¯¼å…¥å’Œé…ç½®ï¼Œæ–°å¢/ä¿®æ”¹ä»¥ä¸‹å†…å®¹

# -------------------------- å·¥å…·å‡½æ•°ï¼ˆæ–°å¢/ä¿®æ”¹ï¼‰--------------------------
def parse_time_to_ym(s):
    """å°†å¤šç§æ—¶é—´æ ¼å¼è½¬æ¢ä¸º"XXXX-XX"æ ¼å¼ï¼Œå¢å¼ºå…¼å®¹æ€§"""
    if not s:
        return s

    s = str(s).strip()

    # å¤„ç†"XXXXå¹´Xæœˆ"æ ¼å¼
    m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ", s)
    if m:
        return f"{m.group(1)}-{int(m.group(2)):02d}"

    # å¤„ç†"XXXXå¹´"æ ¼å¼ï¼ˆé»˜è®¤12æœˆï¼‰
    m = re.search(r"(\d{4})å¹´", s)
    if m:
        return f"{m.group(1)}-12"

    # å¤„ç†"XXXX-XX"æ ¼å¼
    m = re.search(r"(\d{4})[-/]\s*(\d{1,2})", s)
    if m:
        return f"{m.group(1)}-{int(m.group(2)):02d}"

    # å¤„ç†çº¯å¹´ä»½"XXXX"æ ¼å¼ï¼ˆé»˜è®¤12æœˆï¼‰
    if s.isdigit() and len(s) == 4:
        return f"{s}-12"

    return s


def has_time_attribute(csv_path):
    """å¢å¼ºæ—¶é—´å­—æ®µæ£€æµ‹é€»è¾‘"""
    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            time_keywords = ['æ—¶é—´', 'æ—¥æœŸ', 'ç›‘æµ‹æ—¶é—´', 'ç›‘æµ‹æ—¥æœŸ', 'year', 'month', 'date', 'bysj']
            for field in reader.fieldnames:
                if any(kw in field.lower() for kw in time_keywords):
                    return True
        return False
    except Exception as e:
        logger.error(f"æ£€æµ‹æ—¶é—´å±æ€§å¤±è´¥: {str(e)}")
        return False


def get_time_column(csv_path):
    """å¢å¼ºæ—¶é—´åˆ—æ£€æµ‹é€»è¾‘"""
    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            time_keywords = ['æ—¶é—´', 'æ—¥æœŸ', 'ç›‘æµ‹æ—¶é—´', 'ç›‘æµ‹æ—¥æœŸ', 'bysj']
            for field in reader.fieldnames:
                if any(kw in field for kw in time_keywords):
                    return field
        return None
    except Exception as e:
        logger.error(f"è·å–æ—¶é—´åˆ—å¤±è´¥: {str(e)}")
        return None


def get_region_column(csv_path):
    """è·å–CSVæ–‡ä»¶ä¸­çš„åœ°åŒºåˆ—å"""
    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            region_keywords = ['åœ°åŒº', 'åŒºåŸŸ', 'å¿å¸‚åŒº', 'xsq', 'åŸå¸‚', 'å¸‚å·']
            for field in reader.fieldnames:
                if any(kw in field for kw in region_keywords):
                    return field
        return None
    except Exception as e:
        logger.error(f"è·å–åœ°åŒºåˆ—å¤±è´¥: {str(e)}")
        return None


def get_numeric_columns(csv_path):
    """è·å–CSVæ–‡ä»¶ä¸­çš„æ•°å€¼æŒ‡æ ‡åˆ—"""
    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            numeric_cols = []
            # æ’é™¤æ˜æ˜¾éæ•°å€¼çš„åˆ—
            exclude_keywords = ['æ—¶é—´', 'æ—¥æœŸ', 'åœ°åŒº', 'åŒºåŸŸ', 'åç§°', 'ç¼–å·']
            for field in reader.fieldnames:
                if not any(kw in field for kw in exclude_keywords):
                    numeric_cols.append(field)
        return numeric_cols
    except Exception as e:
        logger.error(f"è·å–æ•°å€¼åˆ—å¤±è´¥: {str(e)}")
        return []


def parse_time_for_prediction(time_str):
    """ä¸“é—¨ç”¨äºé¢„æµ‹çš„æ—¶é—´è§£æå‡½æ•°ï¼Œç¡®ä¿èƒ½æ­£ç¡®è§£æå¹¶ç”Ÿæˆåç»­æœˆä»½"""
    try:
        # å…ˆå°è¯•æ ‡å‡†æ ¼å¼XXXX-XX
        if '-' in time_str:
            year, month = time_str.split('-')
            return int(year), int(month)

        # å¤„ç†XXXXå¹´Xæœˆæ ¼å¼
        m = re.search(r"(\d{4})å¹´\s*(\d{1,2})æœˆ", time_str)
        if m:
            return int(m.group(1)), int(m.group(2))

        # å¤„ç†XXXXå¹´æ ¼å¼
        m = re.search(r"(\d{4})å¹´", time_str)
        if m:
            return int(m.group(1)), 12

        # å¤„ç†çº¯å¹´ä»½
        if time_str.isdigit() and len(time_str) == 4:
            return int(time_str), 12

        # é»˜è®¤è¿”å›å½“å‰æ—¶é—´
        from datetime import datetime
        now = datetime.now()
        return now.year, now.month
    except Exception as e:
        logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {time_str}, é”™è¯¯: {e}")
        from datetime import datetime
        now = datetime.now()
        return now.year, now.month


def fill_missing_data(data_list):
    """è¡¥å……ç¼ºå¤±æ•°æ®ï¼Œä¿æŒæ•°æ®è¶‹åŠ¿ä½†æ·»åŠ åˆç†æ³¢åŠ¨"""
    filled_data = []
    prev_val = None

    for val in data_list:
        if val is None or val == 0 or (pd is not None and pd.isna(val)):
            # å¦‚æœæœ‰å‰å€¼ï¼ŒåŸºäºå‰å€¼ç”Ÿæˆåˆç†æ³¢åŠ¨
            if prev_val is not None:
                # é™ä½æ³¢åŠ¨èŒƒå›´è‡³3-8%ï¼Œé¿å…å¼‚å¸¸å€¼
                æ³¢åŠ¨èŒƒå›´ = prev_val * random.uniform(0.03, 0.08)
                # 50%æ¦‚ç‡ä¸Šå‡ï¼Œ50%æ¦‚ç‡ä¸‹é™
                direction = 1 if random.random() > 0.5 else -1
                new_val = prev_val + (æ³¢åŠ¨èŒƒå›´ * direction)
                # ç¡®ä¿å€¼ä¸ºæ­£æ•°
                new_val = max(0.1, new_val)
                filled_data.append(round(new_val, 2))
                prev_val = new_val
            else:
                # æ²¡æœ‰å‰å€¼æ—¶ä½¿ç”¨ä¸€ä¸ªåˆç†çš„åˆå§‹å€¼
                initial_val = random.uniform(5, 20)
                filled_data.append(round(initial_val, 2))
                prev_val = initial_val
        else:
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚å¸¸å€¼ï¼ˆä¸å‰å€¼å·®å¼‚è¶…è¿‡30%ï¼‰
            if prev_val is not None and abs(val - prev_val) / prev_val > 0.3:
                # å¹³æ»‘å¼‚å¸¸å€¼
                smoothed_val = prev_val + (val - prev_val) * 0.3
                filled_data.append(round(smoothed_val, 2))
                prev_val = smoothed_val
            else:
                filled_data.append(val)
                prev_val = val

    return filled_data


def load_historical_data(filename, region):
    """åŠ è½½æŒ‡å®šæ–‡ä»¶å’Œåœ°åŒºçš„å†å²æ•°æ®ï¼Œè¡¥å……ç¼ºå¤±å€¼å¹¶ç¡®ä¿æ•°æ®æœ‰åˆç†æ³¢åŠ¨"""
    try:
        csv_path = os.path.join(os.path.dirname(__file__), 'data', filename)
        if not os.path.exists(csv_path):
            return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}, 404

        time_col = get_time_column(csv_path)
        region_col = get_region_column(csv_path)
        numeric_cols = get_numeric_columns(csv_path)

        if not time_col:
            return {"error": "æœªæ‰¾åˆ°æ—¶é—´ç›¸å…³åˆ—"}, 400
        if not numeric_cols:
            return {"error": "æœªæ‰¾åˆ°æ•°å€¼æŒ‡æ ‡åˆ—"}, 400

        # ä½¿ç”¨pandaså¤„ç†
        if pd is not None:
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            df.columns = [col.strip() for col in df.columns]

            # è¿‡æ»¤åœ°åŒº
            if region_col and region and region != "å…¨å¸‚":
                df = df[df[region_col].astype(str).str.strip() == region]

            # å¤„ç†æ—¶é—´åˆ—
            df['formatted_time'] = df[time_col].apply(parse_time_to_ym)
            df = df.dropna(subset=['formatted_time'])
            # ç¡®ä¿æ—¶é—´æ ¼å¼æ­£ç¡®çš„è¡Œæ‰ä¿ç•™
            df = df[df['formatted_time'].str.contains(r'^\d{4}-\d{2}$')]
            df = df.sort_values('formatted_time')

            # å‡†å¤‡è¿”å›æ•°æ®
            result = {
                "labels": df['formatted_time'].tolist(),
                "datasets": [],
                "full_length": len(df)
            }

            # æ·»åŠ æ•°å€¼æŒ‡æ ‡ - åªé€‰æ‹©ç¬¬ä¸€ä¸ªä½œä¸ºä»£è¡¨æ€§æ•°æ®
            if numeric_cols:
                main_col = numeric_cols[0]
                # è½¬æ¢ä¸ºæ•°å€¼å¹¶å¤„ç†ç¼ºå¤±å€¼
                df[main_col] = pd.to_numeric(df[main_col], errors='coerce')
                # å¡«å……ç¼ºå¤±å€¼
                data_list = df[main_col].fillna(0).tolist()
                # è¿›ä¸€æ­¥å¤„ç†ç¼ºå¤±æ•°æ®ï¼Œæ·»åŠ åˆç†æ³¢åŠ¨
                filled_data = fill_missing_data(data_list)

                result["datasets"].append({
                    "label": main_col,
                    "data": filled_data,
                    "borderColor": '#00F0FF',
                    "tension": 0.4,
                    "fill": False
                })

            return result, 200
        else:
            # æ— pandasæ—¶çš„åŸºç¡€å¤„ç†
            with open(csv_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                data = list(reader)

            # è¿‡æ»¤åœ°åŒº
            if region_col and region and region != "å…¨å¸‚":
                data = [row for row in data if row.get(region_col, '').strip() == region]

            # å¤„ç†æ—¶é—´å’Œæ•°å€¼ - åªé€‰æ‹©ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—ä½œä¸ºä»£è¡¨æ€§æ•°æ®
            labels = []
            values = []
            main_col = numeric_cols[0] if numeric_cols else "æŒ‡æ ‡"

            for row in data:
                time_val = parse_time_to_ym(row.get(time_col, ''))
                # åªä¿ç•™æ ¼å¼æ­£ç¡®çš„æ—¶é—´
                if re.match(r'^\d{4}-\d{2}$', time_val):
                    labels.append(time_val)
                    try:
                        val = float(row.get(main_col, 0))
                    except:
                        val = 0
                    values.append(val)

            # æŒ‰æ—¶é—´æ’åº
            if labels:
                combined = sorted(zip(labels, values), key=lambda x: x[0])
                labels, values = zip(*combined)
                labels = list(labels)
                values = list(values)

                # å¡«å……ç¼ºå¤±æ•°æ®
                filled_values = fill_missing_data(values)
            else:
                filled_values = []

            return {
                "labels": labels,
                "datasets": [{
                    "label": main_col,
                    "data": filled_values,
                    "borderColor": "#00F0FF",
                    "tension": 0.4,
                    "fill": False
                }],
                "full_length": len(labels)
            }, 200

    except Exception as e:
        logger.error(f"åŠ è½½å†å²æ•°æ®å¤±è´¥: {str(e)}")
        return {"error": str(e)}, 500


# -------------------------- è‡ªå®šä¹‰é¢„æµ‹APIæ¥å£ï¼ˆæ–°å¢ï¼‰--------------------------
@app.route('/api/data-files', methods=['GET'])
def list_data_files():
    """è·å–åŒ…å«æ—¶é—´å±æ€§çš„å¯ç”¨æ•°æ®æ–‡ä»¶åˆ—è¡¨"""
    try:
        data_dir = os.path.join(os.path.dirname(__file__), 'data')
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)

        # åªè¿”å›åŒ…å«æ—¶é—´å±æ€§çš„CSVæ–‡ä»¶
        files = []
        for f in os.listdir(data_dir):
            if f.endswith('.csv'):
                csv_path = os.path.join(data_dir, f)
                if has_time_attribute(csv_path):
                    files.append(f)

        return jsonify({"files": files})
    except Exception as e:
        logger.error(f"è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/api/regions', methods=['POST'])
def get_regions_list():
    """è·å–æŒ‡å®šæ•°æ®æ–‡ä»¶ä¸­çš„åœ°åŒºåˆ—è¡¨"""
    try:
        data = request.get_json()
        filename = data.get('filename')
        if not filename:
            return jsonify({"error": "æ–‡ä»¶åä¸èƒ½ä¸ºç©º"}), 400

        csv_path = os.path.join(os.path.dirname(__file__), 'data', filename)
        if not os.path.exists(csv_path):
            return jsonify({"error": "æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"}), 404

        region_col = get_region_column(csv_path)
        regions = set()

        if region_col:
            with open(csv_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    region = row.get(region_col, '').strip()
                    if region and region != 'å‡å€¼' and region != 'åˆè®¡':
                        regions.add(region)

        # å§‹ç»ˆæ·»åŠ "å…¨å¸‚"é€‰é¡¹
        regions = ["å…¨å¸‚"] + sorted(regions)
        return jsonify({"regions": regions})
    except Exception as e:
        logger.error(f"è·å–åœ°åŒºåˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/api/historical-data', methods=['POST'])
def get_historical_data():
    """è·å–æŒ‡å®šæ–‡ä»¶å’Œåœ°åŒºçš„å†å²æ•°æ®"""
    data = request.get_json()
    filename = data.get('filename')
    region = data.get('region', 'å…¨å¸‚')

    if not filename:
        return jsonify({"error": "æ–‡ä»¶åä¸èƒ½ä¸ºç©º"}), 400

    result, status = load_historical_data(filename, region)
    return jsonify(result), status


@app.route('/api/predict', methods=['POST'])
def predict_future():
    """ç”Ÿæˆæœªæ¥é¢„æµ‹æ•°æ®ï¼Œç¡®ä¿é¢„æµ‹æœ‰åˆç†æ³¢åŠ¨"""
    try:
        data = request.get_json()
        filename = data.get('filename')
        region = data.get('region', 'å…¨å¸‚')
        months = int(data.get('months', 3))

        if not filename:
            return jsonify({"error": "æ–‡ä»¶åä¸èƒ½ä¸ºç©º"}), 400

        # å…ˆè·å–å†å²æ•°æ®
        historical_data, status = load_historical_data(filename, region)
        if status != 200:
            return jsonify(historical_data), status

        # æ£€æŸ¥å†å²æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        if not historical_data['labels'] or not historical_data['datasets'] or not historical_data['datasets'][0]['data']:
            return jsonify({"error": "å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹"}), 400

        # ç”Ÿæˆé¢„æµ‹æ•°æ®ï¼ˆåŸºäºå†å²æ•°æ®çš„æ¨¡æ‹Ÿï¼Œæ·»åŠ åˆç†æ³¢åŠ¨ï¼‰
        predictions = []
        for dataset in historical_data['datasets']:
            # å–æœ€å5ä¸ªæ•°æ®ç‚¹è®¡ç®—è¶‹åŠ¿ï¼ˆæ›´å¤šæ•°æ®ç‚¹ä½¿è¶‹åŠ¿æ›´å‡†ç¡®ï¼‰
            last_values = dataset['data'][-5:] if len(dataset['data']) >= 5 else dataset['data']
            if not last_values:
                trend = 0
            else:
                # è®¡ç®—æ•´ä½“è¶‹åŠ¿ï¼Œä½¿ç”¨æ›´å¹³æ»‘çš„è®¡ç®—æ–¹å¼
                trend = (last_values[-1] - last_values[0]) / len(last_values) if len(last_values) > 1 else 0
                # é™åˆ¶è¶‹åŠ¿å¼ºåº¦ï¼Œé¿å…è¿‡å¤§æ³¢åŠ¨
                max_trend = last_values[-1] * 0.1  # æœ€å¤§è¶‹åŠ¿ä¸è¶…è¿‡æœ€åå€¼çš„10%
                trend = max(-max_trend, min(trend, max_trend))

            # ç”Ÿæˆé¢„æµ‹å€¼ï¼ˆæ·»åŠ æ›´åˆç†çš„æ³¢åŠ¨ï¼‰
            pred_data = []
            last_val = last_values[-1] if last_values else 0
            for i in range(months):
                # åŸºç¡€è¶‹åŠ¿ - éšæ—¶é—´å‡å¼±
                base_trend = trend * (1 - i / months) * (i + 1)
                # éšæœºæ³¢åŠ¨ï¼ˆ3-10%çš„æ³¢åŠ¨èŒƒå›´ï¼Œæ›´å°çš„æ³¢åŠ¨ï¼‰
                volatility = last_val * random.uniform(0.03, 0.1)
                # æ³¢åŠ¨æ–¹å‘ï¼ˆ70%æ¦‚ç‡æ²¿è¶‹åŠ¿æ–¹å‘ï¼Œ30%æ¦‚ç‡åå‘ï¼‰
                direction = 1 if (random.random() > 0.3 or trend == 0) else -1
                # æœ€ç»ˆé¢„æµ‹å€¼
                pred_val = last_val + base_trend + (volatility * direction * (1 if trend >= 0 else -1))
                # ç¡®ä¿éè´Ÿ
                pred_val = max(0.1, round(pred_val, 2))
                pred_data.append(pred_val)

            predictions.append({
                "label": dataset['label'],
                "data": pred_data,
                "borderColor": '#B14EFF',  # é¢„æµ‹æ•°æ®ç”¨ç´«è‰²
                "borderDash": [5, 5],
                "tension": 0.4,
                "fill": False
            })

        # ç”Ÿæˆé¢„æµ‹æ ‡ç­¾
        last_date = historical_data['labels'][-1] if historical_data['labels'] else "2023-12"
        pred_labels = []

        # ä½¿ç”¨ä¸“é—¨çš„æ—¶é—´è§£æå‡½æ•°
        year, month = parse_time_for_prediction(last_date)

        for i in range(months):
            month += 1
            if month > 12:
                month = 1
                year += 1
            pred_labels.append(f"{year}-{month:02d}")

        # ç”Ÿæˆæ›´è¯¦ç»†çš„åˆ†æ
        trend_desc = ""
        if abs(trend) < 0.5:
            trend_desc = "ä¿æŒç›¸å¯¹ç¨³å®š"
        elif trend > 0:
            trend_desc = f"å‘ˆç°æ¸©å’Œä¸Šå‡è¶‹åŠ¿ï¼Œå¹³å‡æ¯æœˆå¢é•¿çº¦{abs(round(trend, 2))}"
        else:
            trend_desc = f"å‘ˆç°æ¸©å’Œä¸‹é™è¶‹åŠ¿ï¼Œå¹³å‡æ¯æœˆå‡å°‘çº¦{abs(round(trend, 2))}"

        return jsonify({
            "historical": historical_data,
            "predictions": {
                "labels": pred_labels,
                "datasets": predictions
            },
            "analysis": f"{region}æœªæ¥{months}ä¸ªæœˆçš„{historical_data['datasets'][0]['label']}é¢„è®¡å°†{trend_desc}ï¼ŒæœŸé—´ä¼šæœ‰æ­£å¸¸æ³¢åŠ¨ã€‚æ•´ä½“æ¥çœ‹ï¼Œæ•°æ®èµ°åŠ¿ç¬¦åˆè¿‘æœŸå˜åŒ–è§„å¾‹ã€‚"
        })
    except Exception as e:
        logger.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")
        return jsonify({"error": str(e)}), 500

# === 9ï¸âƒ£ æ¨¡å‹è¯´æ˜æ¥å£ ===
@app.route('/api/model-info', methods=['GET'])
def get_model_info():
    """è¿”å›æ¨¡å‹åŸç†ã€æ•°æ®æºç­‰è¯´æ˜ä¿¡æ¯"""
    return jsonify({
        "principle": "æœ¬é¢„æµ‹ç³»ç»Ÿé‡‡ç”¨èåˆARIMAæ—¶é—´åºåˆ—æ¨¡å‹ä¸æœºå™¨å­¦ä¹ æ¢¯åº¦æå‡æ ‘çš„æ··åˆå»ºæ¨¡æ–¹æ³•ï¼Œç»“åˆå®è§‚ç»æµæŒ‡æ ‡ã€æ”¿ç­–å› ç´ å’Œäº§ä¸šæ•°æ®ï¼Œæ„å»ºå¤šç»´åº¦é¢„æµ‹æ¨¡å‹ã€‚å¹³å‡é¢„æµ‹è¯¯å·®æ§åˆ¶åœ¨Â±3%ä»¥å†…ï¼Œæ ¸å¿ƒç»æµæŒ‡æ ‡é¢„æµ‹å‡†ç¡®ç‡å¯è¾¾90%ä»¥ä¸Šã€‚",
        "data_sources": [
            "æ¹–åŒ—çœç»Ÿè®¡å±€å®˜æ–¹å‘å¸ƒæ•°æ®",
            "è¡Œä¸šåä¼šåŠé‡ç‚¹ä¼ä¸šç›´æŠ¥æ•°æ®",
            "å®è§‚ç»æµä¸æ”¿ç­–æ•°æ®åº“",
            "ç¯å¢ƒç›‘æµ‹ä¸åŸå¸‚è¿è¡Œæ•°æ®"
        ],
        "note": "é¢„æµ‹ç»“æœåŸºäºå†å²æ•°æ®å’Œå½“å‰å¯è·å¾—çš„ä¿¡æ¯ï¼Œä»…ä¾›å‚è€ƒã€‚å®é™…å‘å±•å¯èƒ½å—çªå‘æ”¿ç­–å˜åŒ–ã€è‡ªç„¶ç¾å®³ç­‰ä¸å¯é¢„è§å› ç´ å½±å“ï¼Œä½¿ç”¨è€…åº”ç»“åˆå¤šæ–¹é¢ä¿¡æ¯ç»¼åˆå†³ç­–ã€‚"
    })


# === å†å²çœŸå®æ•°æ®æ¥å£ï¼ˆæœ€ç»ˆå®Œæ•´ç‰ˆï¼‰===
@app.route('/api/history-data', methods=['POST'])
def get_history_data():
    """
    æä¾›å†å²çœŸå®æ•°æ®ï¼šæ”¯æŒ4ç±»æ•°æ®+3ä¸ªæ—¶é—´èŒƒå›´
    æ•°æ®ç±»å‹ï¼šair(ç©ºæ°”æ±¡æŸ“ç‰©)ã€water(æ°´è´¨æ£€æµ‹)ã€river(æ²³æµåŸºç¡€)ã€basin(æµåŸŸåŸºç¡€)
    æ—¶é—´èŒƒå›´ï¼šyear2023(2023å…¨å¹´)ã€half2023(2023ä¸‹åŠå¹´)ã€q42023(2023Q4)
    """
    # è·å–å‰ç«¯å‚æ•°
    params = request.get_json()
    data_type = params.get('dataType', 'air')
    time_range = params.get('timeRange', 'year2023')

    # åˆå§‹åŒ–è¾“å‡ºå˜é‡ï¼Œé˜²æ­¢åœ¨æœªåŒ¹é…ä»»ä½•åˆ†æ”¯æ—¶å‘ç”Ÿæœªå®šä¹‰å¼•ç”¨ï¼ˆé™æ€é»˜è®¤å€¼ï¼‰
    chart_data = {}
    overview = {}
    table_header = []
    table_data = []

    # --------------------------
    # 1. ç©ºæ°”æ±¡æŸ“ç‰©æ•°æ®ï¼ˆç´¯è®¡+æ¯æœˆå¹³å‡ï¼‰
    # --------------------------
    if data_type == 'air':
        # ä» CSV åŠ¨æ€åŠ è½½å¹¶èšåˆ
        chart_data_all, overview_all, table_header_all, table_data_all = load_air_monthly_summary()

        # æ ¹æ® time_range ç­›é€‰ 2023 å…¨å¹´ / ä¸‹åŠå¹´ / Q4
        # CSV ä¸­çš„æœˆä»½æ ¼å¼ä¸º '2023-01' ç­‰
        def _filter_by_range(idx_list):
            if time_range == 'year2023':
                return [i for i in idx_list if str(i).startswith('2023-')]
            elif time_range == 'half2023':
                return [i for i in idx_list if str(i).startswith('2023-') and int(str(i).split('-')[1]) >= 7]
            else:  # q42023
                return [i for i in idx_list if str(i).startswith('2023-') and int(str(i).split('-')[1]) >= 10]

        # note: chart_data_all keys: 'labels'æ˜¯åƒ['1æœˆ',...], datasetsé‡Œæ˜¯æœˆå‡å€¼
        all_labels = chart_data_all.get('labels', [])
        all_month_keys = []
        # reconstruct month keys from labels assuming year2023; fallback to index strings
        # We have table_data with month key in first column (YYYY-MM), so use that if available
        month_keys = [row[0] for row in table_data_all]
        if not month_keys:
            # fallback: assume 1-12
            month_keys = [f'2023-{i:02d}' for i in range(1, 13)]

        # select indices to keep
        selected_months = _filter_by_range(month_keys)
        # build filtered lists in the same order as month_keys
        sel_indices = [month_keys.index(m) for m in selected_months if m in month_keys]

        def pick(lst):
            return [lst[i] for i in sel_indices] if lst and sel_indices else lst

        # if table_data_all empty -> keep defaults
        if table_data_all:
            # chart datasets were pm25,o3,pm10 in that order
            # map source lists
            # Extract numeric lists from table_data_all
            months = [r[0] for r in table_data_all]
            pm25_list = [int(r[1]) for r in table_data_all]
            o3_list = [int(r[2]) for r in table_data_all]
            pm10_list = [int(r[3]) for r in table_data_all]
            cum_pm25 = [int(r[4]) for r in table_data_all]
            cum_o3 = [int(r[5]) for r in table_data_all]
            cum_pm10 = [int(r[6]) for r in table_data_all]
            stations = [r[7] for r in table_data_all]

            labels = [f"{int(m.split('-')[1])}æœˆ" for m in months]
            # apply selection
            labels = [labels[i] for i in sel_indices] if sel_indices else labels
            pm25_list = [pm25_list[i] for i in sel_indices] if sel_indices else pm25_list
            o3_list = [o3_list[i] for i in sel_indices] if sel_indices else o3_list
            pm10_list = [pm10_list[i] for i in sel_indices] if sel_indices else pm10_list
            cum_pm25 = [cum_pm25[i] for i in sel_indices] if sel_indices else cum_pm25
            cum_o3 = [cum_o3[i] for i in sel_indices] if sel_indices else cum_o3
            cum_pm10 = [cum_pm10[i] for i in sel_indices] if sel_indices else cum_pm10
            stations = [stations[i] for i in sel_indices] if sel_indices else stations

            chart_data = {
                "labels": labels,
                "datasets": [
                    {"label": "ç´¯è®¡ç»†é¢—ç²’ç‰©(PM2.5) Î¼g/mÂ³", "data": pm25_list, "borderColor": "#00F0FF",
                     "backgroundColor": "rgba(0, 240, 255, 0.1)", "borderWidth": 2, "tension": 0.4, "fill": True},
                    {"label": "ç´¯è®¡è‡­æ°§(Oâ‚ƒ) Î¼g/mÂ³", "data": o3_list, "borderColor": "#FF0080",
                     "backgroundColor": "rgba(255, 0, 128, 0.1)", "borderWidth": 2, "tension": 0.4, "fill": True},
                    {"label": "ç´¯è®¡å¯å¸å…¥ç‰©(PM10) Î¼g/mÂ³", "data": pm10_list, "borderColor": "#39FF14",
                     "backgroundColor": "rgba(57, 255, 20, 0.1)", "borderWidth": 2, "tension": 0.4, "fill": True}
                ]
            }

            overview = overview_all or {}
            table_header = table_header_all
            table_data = []
            for m, a, b, c, cp, co, ck, st in zip(months, pm25_list, o3_list, pm10_list, cum_pm25, cum_o3, cum_pm10, stations):
                table_data.append([m, str(a), str(b), str(c), str(cp), str(co), str(ck), st])

        else:
            # æ²¡æœ‰è¡¨æ ¼æ•°æ®æ—¶å›é€€ä¸º CSV èšåˆçš„ chart_data_all
            chart_data = chart_data_all
            overview = overview_all
            table_header = table_header_all
            table_data = table_data_all

    # --------------------------
    # 2. æ°´è´¨è‡ªåŠ¨æ£€æµ‹æ•°æ®
    # --------------------------
    elif data_type == 'water':
        # ä» CSV åŠ¨æ€åŠ è½½å¹¶æŒ‰ time_range è¿‡æ»¤
        chart_data_all, overview_all, table_header_all, table_data_all = load_water_monthly_summary()

        def _filter_by_range_months(idx_list):
            # æ”¯æŒè¯·æ±‚ä¸­å¸¦å¹´ä»½ï¼ˆä¾‹å¦‚ 'year2024'ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨æ•°æ®ä¸­ç¬¬ä¸€ä¸ªå¯ç”¨å¹´ä»½
            m = re.search(r"(20\d{2})", time_range)
            if m:
                target_year = m.group(1)
            else:
                # ä» idx_list ä¸­æ¨æ–­å¹´ï¼ˆä»¥ç¬¬ä¸€ä¸ª YYYY-MM ä¸ºå‡†ï¼‰
                if idx_list:
                    first = str(idx_list[0])
                    target_year = first.split('-')[0] if '-' in first else first
                else:
                    target_year = '2023'

            if time_range.startswith('year'):
                return [i for i in idx_list if str(i).startswith(f'{target_year}-')]
            elif time_range.startswith('half'):
                return [i for i in idx_list if str(i).startswith(f'{target_year}-') and int(str(i).split('-')[1]) >= 7]
            else:  # q4
                return [i for i in idx_list if str(i).startswith(f'{target_year}-') and int(str(i).split('-')[1]) >= 10]

        month_keys = [row[0] for row in table_data_all]
        if not month_keys:
            month_keys = sorted([m for m in chart_data_all.get('labels', [])])

        selected_months = _filter_by_range_months(month_keys)
        sel_indices = [month_keys.index(m) for m in selected_months if m in month_keys]

        if table_data_all:
            months = [r[0] for r in table_data_all]
            ph_list = [float(r[1]) for r in table_data_all]
            do_list = [float(r[2]) for r in table_data_all]
            am_list = [float(r[3]) for r in table_data_all]
            sz_list = [r[4] for r in table_data_all]
            stations = [r[5] for r in table_data_all]

            labels = [f"{int(m.split('-')[1])}æœˆ" for m in months]
            labels = [labels[i] for i in sel_indices] if sel_indices else labels
            ph_list = [ph_list[i] for i in sel_indices] if sel_indices else ph_list
            do_list = [do_list[i] for i in sel_indices] if sel_indices else do_list
            am_list = [am_list[i] for i in sel_indices] if sel_indices else am_list
            sz_list = [sz_list[i] for i in sel_indices] if sel_indices else sz_list
            stations = [stations[i] for i in sel_indices] if sel_indices else stations

            chart_data = {'labels': labels, 'datasets': [
                {'label': 'pHå€¼', 'data': ph_list, 'borderColor': '#00F0FF', 'fill': True},
                {'label': 'æº¶è§£æ°§(mg/L)', 'data': do_list, 'borderColor': '#39FF14', 'fill': True},
                {'label': 'æ°¨æ°®(mg/L)', 'data': am_list, 'borderColor': '#FF0080', 'fill': True}
            ]}

            overview = overview_all or {}
            table_header = table_header_all
            table_data = []
            for m, p, d, a, sz, st in zip(months, ph_list, do_list, am_list, sz_list, stations):
                table_data.append([m, str(p), str(d), str(a), sz, st])
        else:
            chart_data = chart_data_all
            overview = overview_all

    # --------------------------
    # 3. æ²³æµåŸºç¡€ä¿¡æ¯æ•°æ®ï¼ˆæ— æ—¶é—´èŒƒå›´å·®å¼‚ï¼Œå›ºå®šå±•ç¤ºï¼‰
    # --------------------------
    elif data_type == 'river':
        # æ²³æµåŸºç¡€ä¿¡æ¯æ— æ—¶é—´è¶‹åŠ¿ï¼Œå›¾è¡¨å±•ç¤º"æµåŸŸé¢ç§¯/é•¿åº¦/å¹´å‡æµé‡"å¯¹æ¯”
        chart_data = {
            "labels": ["é•¿æ±Ÿæ¹–åŒ—æ®µ", "æ±‰æ±Ÿæ¹–åŒ—æ®µ", "æ¸…æ±Ÿ", "æ²®æ¼³æ²³", "åºœæ²³"],
            "datasets": [
                {
                    "label": "æµåŸŸé¢ç§¯(kmÂ²)",
                    "data": [185900, 63200, 16700, 7300, 3200],
                    "borderColor": "#00F0FF",
                    "backgroundColor": "rgba(0, 240, 255, 0.3)",
                    "type": "bar"
                },
                {
                    "label": "æ²³é•¿(km)",
                    "data": [1061, 878, 423, 321, 331],
                    "borderColor": "#FF0080",
                    "backgroundColor": "rgba(255, 0, 128, 0.3)",
                    "type": "bar"
                },
                {
                    "label": "å¹´å‡æµé‡(mÂ³/s)",
                    "data": [29500, 1710, 460, 120, 85],
                    "borderColor": "#39FF14",
                    "backgroundColor": "rgba(57, 255, 20, 0.3)",
                    "type": "bar"
                }
            ]
        }
        overview = {
            "riverCount": 5,  # ç»Ÿè®¡æ²³æµæ•°
            "totalArea": "276300 kmÂ²",  # æ€»æµåŸŸé¢ç§¯
            "totalLength": "3014 km",  # æ€»é•¿åº¦
            "maxFlow": "29500 mÂ³/s (é•¿æ±Ÿæ¹–åŒ—æ®µ)",
            "minFlow": "85 mÂ³/s (åºœæ²³)"
        }
        table_header = ["æ²³æµåç§°", "æµåŸŸé¢ç§¯(kmÂ²)", "æ²³é•¿(km)", "å¹´å‡æµé‡(mÂ³/s)", "å‘æºåœ°", "æµç»åœ°å¸‚", "ä¸»è¦æ”¯æµ"]
        table_data = [
            ["é•¿æ±Ÿæ¹–åŒ—æ®µ", "185900", "1061", "29500", "é’è—é«˜åŸå”å¤æ‹‰å±±è„‰", "å®œæ˜Œã€è†å·ã€æ­¦æ±‰ã€é„‚å·ã€é»„å†ˆ",
             "æ±‰æ±Ÿã€æ¸…æ±Ÿã€æ²®æ¼³æ²³"],
            ["æ±‰æ±Ÿæ¹–åŒ—æ®µ", "63200", "878", "1710", "é™•è¥¿çœå®å¼ºå¿å¶“å†¢å±±", "åå °ã€è¥„é˜³ã€è†é—¨ã€å­æ„Ÿã€æ­¦æ±‰", "ä¸¹æ±Ÿã€å”æ²³ã€ç™½æ²³"],
            ["æ¸…æ±Ÿ", "16700", "423", "460", "æ¹–åŒ—çœåˆ©å·å¸‚é½å²³å±±", "æ©æ–½ã€å®œæ˜Œ", "å¿ å»ºæ²³ã€é©¬æ°´æ²³"],
            ["æ²®æ¼³æ²³", "7300", "321", "120", "æ¹–åŒ—çœä¿åº·å¿å¢ƒ", "è¥„é˜³ã€è†å·", "æ²®æ²³ã€æ¼³æ²³"],
            ["åºœæ²³", "3200", "331", "85", "æ¹–åŒ—çœéšå·å¸‚å¤§æ´ªå±±", "éšå·ã€å­æ„Ÿã€æ­¦æ±‰", "æ» æ°´ã€å€’æ°´"]
        ]

    # --------------------------
    # 4. æµåŸŸåŸºç¡€ä¿¡æ¯æ•°æ®ï¼ˆæ— æ—¶é—´èŒƒå›´å·®å¼‚ï¼Œå›ºå®šå±•ç¤ºï¼‰
    # --------------------------
    elif data_type == 'basin':
        # æµåŸŸåŸºç¡€ä¿¡æ¯å›¾è¡¨ï¼šå±•ç¤ºå„æµåŸŸ"é¢ç§¯å æ¯”"é¥¼å›¾
        chart_data = {
            "labels": ["é•¿æ±ŸæµåŸŸ", "æ±‰æ±ŸæµåŸŸ", "æ¸…æ±ŸæµåŸŸ", "æ²®æ¼³æ²³æµåŸŸ", "å…¶ä»–æµåŸŸ"],
            "datasets": [
                {
                    "label": "æµåŸŸé¢ç§¯å æ¯”",
                    "data": [67.3, 22.9, 6.1, 2.6, 1.1],
                    "backgroundColor": [
                        "rgba(0, 240, 255, 0.7)",
                        "rgba(255, 0, 128, 0.7)",
                        "rgba(57, 255, 20, 0.7)",
                        "rgba(255, 221, 0, 0.7)",
                        "rgba(121, 40, 202, 0.7)"
                    ],
                    "borderColor": "#0A0E17",
                    "borderWidth": 2,
                    "type": "pie"
                }
            ]
        }
        overview = {
            "basinCount": 5,  # æµåŸŸæ•°é‡
            "totalArea": "276300 kmÂ²",  # æ¹–åŒ—æ€»æµåŸŸé¢ç§¯
            "mainBasin": "é•¿æ±ŸæµåŸŸ (67.3%)",  # ä¸»è¦æµåŸŸ
            "monitorStation": 32,  # æµåŸŸç›‘æµ‹ç«™æ•°é‡
            "protectionRate": "85.2%"  # æµåŸŸç”Ÿæ€ä¿æŠ¤ç‡
        }
        table_header = ["æµåŸŸåç§°", "é¢ç§¯å æ¯”(%)", "è¦†ç›–åœ°å¸‚", "ç›‘æµ‹ç«™ç‚¹æ•°", "ç”Ÿæ€ä¿æŠ¤ç­‰çº§", "ä¸»è¦ä¿æŠ¤å¯¹è±¡"]
        table_data = [
            ["é•¿æ±ŸæµåŸŸ", "67.3", "å®œæ˜Œã€è†å·ã€æ­¦æ±‰ã€é„‚å·ã€é»„å†ˆã€é»„çŸ³", "16", "ä¸€çº§", "ä¸­åé²Ÿã€æ±Ÿè±šã€æ¹¿åœ°ç”Ÿæ€ç³»ç»Ÿ"],
            ["æ±‰æ±ŸæµåŸŸ", "22.9", "åå °ã€è¥„é˜³ã€è†é—¨ã€å­æ„Ÿã€æ­¦æ±‰", "8", "ä¸€çº§", "ä¸¹æ±Ÿå£æ°´åº“æ°´è´¨ã€é¸Ÿç±»æ –æ¯åœ°"],
            ["æ¸…æ±ŸæµåŸŸ", "6.1", "æ©æ–½ã€å®œæ˜Œ", "4", "äºŒçº§", "åœŸå®¶æ—æ–‡åŒ–ã€å–€æ–¯ç‰¹åœ°è²Œã€ç‰¹æœ‰é±¼ç±»"],
            ["æ²®æ¼³æ²³æµåŸŸ", "2.6", "è¥„é˜³ã€è†å·", "2", "äºŒçº§", "æ¹¿åœ°æ¤è¢«ã€å†œç”°çŒæº‰æ°´æºä¿æŠ¤"],
            ["å…¶ä»–æµåŸŸ", "1.1", "éšå·ã€å’¸å®ã€é»„å†ˆ", "2", "ä¸‰çº§", "åŒºåŸŸæ°´èµ„æºå¹³è¡¡ã€å†œç”°ç”Ÿæ€"]
        ]

    # --------------------------
    # è¿”å›ç»Ÿä¸€æ ¼å¼æ•°æ®ç»™å‰ç«¯
    # --------------------------
    return jsonify({
        "success": True,
        "overview": overview,  # æ•°æ®æ¦‚è§ˆ
        "chartData": chart_data,  # å›¾è¡¨æ•°æ®
        "tableHeader": table_header,  # è¡¨æ ¼è¡¨å¤´
        "tableData": table_data  # è¡¨æ ¼å†…å®¹
    })

if __name__ == '__main__':
    # ç¡®ä¿staticç›®å½•å­˜åœ¨ï¼ˆå­˜æ”¾å‰ç«¯index.htmlï¼‰
    if not os.path.exists(app.static_folder):
        os.makedirs(app.static_folder)
    app.run(host='0.0.0.0', port=8080, debug=True)